# 学习笔记

[线性回归 — 从0开始](http://zh.gluon.ai/chapter_supervised-learning/linear-regression-scratch.html)


首先创建数据

```
num_inputs = 2
num_examples = 1000

true_w = [2, -3.4]
true_b = 4.2

X = nd.random_normal(shape=(num_examples, num_inputs))
y = true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b
y += .01 * nd.random_normal(shape=y.shape)
```

这里y的计算可以等价于

```
y2 = nd.dot(X, nd.array(true_w)) + true_b;
```

这里生成了输入X，为每行长度为2的向量，而Y是每行长度为1的向量

可以用pyplot绘制这里的X，y，不过X是二维的，所以每次只能取其中1维，另外绘制元素必须是numpy数组。

```
plt.scatter(X[:, 0].asnumpy(), y.asnumpy())
plt.show()
```

再来看数据读取，每次训练时会随机读取一批数据，所以这里要做的是随机打乱数据，循环批量抽取。

```
def data_iter():
    idx = list(range(num_examples))
    random.shuffle(idx)
    for i in range(0, num_examples, batch_size):
        j = nd.array(idx[i:min(i + batch_size, num_examples)])
        yield nd.take(X, j), nd.take(y, j)
```

这里range生成元祖，由于下面要打乱，所以需要先转成列表才能写。打乱后，在for循环中依次批量读出X的每一行和y的每一行。这里yield表示迭代器。采用如下方式就可以不断读取data和label了，data对应的是X每一行向量，label表示对应的y。

```
for data, label in data_iter():
```

接下来初始化模型参数，参数只有w和b，w是1*2的向量，这里初始化为随机值。

```
w = nd.random_normal(shape=(num_inputs, 1))
b = nd.zeros((1,))
params = [w, b]
```

训练的时候要分别对w和b进行求导来更新他们的值，使loss最小，因此这里要创建他们的梯度，

```
for param in params:
    param.attach_grad()
```

接下来定义模型，此处模型就是个线性函数y = wx + b

```
def net(X):
    return nd.dot(X, w) + b
```

接下来定义损失函数，这的yhat应该是标准输出，此处reshape将y转成和yhat一样的矩阵形式，对于一个矩阵C，C ** 2表示每个元素都平方一下。

```
def square_loss(yhat, y):
    # 注意这里我们把y变形成yhat的形状来避免矩阵形状的自动转换
    return (yhat - y.reshape(yhat.shape)) ** 2
```

有了损失函数，接下来看优化，即当前param沿着梯度相反方向移动一步。

```
def SGD(params, lr):
    for param in params:
        param[:] = param - lr * param.grad
```

这里params是w和b组成的数组，w和b都是numpy数组，因此可以相减。另外由于param都attach_grad了，所以其中会保存当前梯度值，此处lr为学习率，或可认为是步长。

接下来开始训练，

```
# 模型函数
def real_fn(X):
    return nd.dot(X, nd.array(true_w)) + true_b

# 绘制损失随训练次数降低的折线图，以及预测值和真实值的散点图
def plot(losses, X, sample_size=100):
    xs = list(range(len(losses)))
    f, (fg1, fg2) = plt.subplots(1, 2)
    fg1.set_title('Loss during training')
    fg1.plot(xs, losses, '-r')
    fg2.set_title('Estimated vs real function')
    fg2.plot(X[:sample_size, 1].asnumpy(),
             net(X[:sample_size, :]).asnumpy(), 'or', label='Estimated')
    fg2.plot(X[:sample_size, 1].asnumpy(),
             real_fn(X[:sample_size, :]).asnumpy(), '*g', label='Real')
    fg2.legend()
    plt.show()
```

这里plt.subplots返回的参数有两个，一个是figure，一个是axis元组，由于此处生成的是1*2的坐标轴，因此返回两个axis。对于fg1，横坐标是xs，losses的个数即为epoch数，因为训练时数据集大小为1000，每次取10个数据，循环100次完成一个epoch，此时plot绘制一次，并append一个loss。fg1的纵坐标为每个epoch的loss。再看fg2，其横坐标为X的前100个数据集的第二维数据，分别绘制出拟合的结果和真实的结果。legend函数是给出两种点的标识。

最后看训练过程，

```
epochs = 5
learning_rate = .001
niter = 0
losses = []
moving_loss = 0
smoothing_constant = .01

# 训练
for e in range(epochs):
    total_loss = 0

    for data, label in data_iter():
        with autograd.record():
            output = net(data)
            loss = square_loss(output, label)
        loss.backward()
        SGD(params, learning_rate)
        total_loss += nd.sum(loss).asscalar()

        # 记录每读取一个数据点后，损失的移动平均值的变化；
        niter +=1
        curr_loss = nd.mean(loss).asscalar()
        moving_loss = (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss

        # correct the bias from the moving averages
        est_loss = moving_loss/(1-(1-smoothing_constant)**niter)

        if (niter + 1) % 100 == 0:
            losses.append(est_loss)
            print("Epoch %s, batch %s. Moving avg of loss: %s. Average loss: %f" % (e, niter, est_loss, total_loss/num_examples))
            plot(losses, X)
```

这里训练过程迭代5次，每次从1000个数据集中通过迭代器依次取10个数据进行拟合，100次完成一轮。再来看求导过程，[官方文档](http://zh.gluon.ai/chapter_crashcourse/autograd.html)有详细说明，步骤如下：

```
x.attach_grad()

with autograd.record():
    y = f(x)

y.backward()  

x.grad  
```

可见最后从x.grad中可取出对应的导数。回到之前的例子，对于每次取出的数据集，都先计算loss，然后求导，再通过SGD向梯度反向递进。这里的误差计算方法就暂时略过。

综上，可看出即便是简单的线性回归也要写这么多代码，确实是件麻烦的事。
